{"cells":[{"cell_type":"code","source":["############################ Feature extraction\n#TF-IDF (HashingTF and IDF)"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n\nsentenceData = sqlContext.createDataFrame([\n    (0, \"Hi I heard about Spark\"),\n    (0, \"I wish Java could use case classes\"),\n    (1, \"Logistic regression models are neat\")\n], [\"label\", \"sentence\"])\nsentenceData.show(truncate=False)"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["# Tokenization is the process of taking text (such as a sentence) and breaking it into individual terms (usually words). A simple Tokenizer class provides this functionality. The example below shows how to split sentences into sequences of words.\n\n# We split each sentence into words using Tokenizer. \ntokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\nwordsData = tokenizer.transform(sentenceData) ## this is a spark data frame, add one more column 'words'\n\nwords = wordsData.select('words').collect() ## select the words column, the type is list. \nwordsData.show(truncate=False)\n\n# RegexTokenizer allows more advanced tokenization based on regular expression (regex) matching. \nfrom pyspark.ml.feature import Tokenizer, RegexTokenizer\nregexTokenizer = RegexTokenizer(inputCol=\"sentence\", outputCol=\"words\", pattern=\"\\\\W\")\nw2 = regexTokenizer.transform(sentenceData)\nw2.show(truncate=False)"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["# TF: HashingTF is a Transformer which takes sets of terms and converts those sets into fixed-length feature vectors. In text processing, a “set of terms” might be a bag of words. The algorithm combines Term Frequency (TF) counts with the hashing trick for dimensionality reduction.\n\n# we use HashingTF to hash the sentence into a feature vector.\nhashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=20)\nfeaturizedData = hashingTF.transform(wordsData) # this returns data frame. \n\nfeaturizedData.show(truncate=False)\nraw = featurizedData.select('rawFeatures').collect() # this returns list. \nfor i in raw:\n  print i"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["# IDF: IDF is an Estimator which fits on a dataset and produces an IDFModel. The IDFModel takes feature vectors (generally created from HashingTF) and scales each column. Intuitively, it down-weights columns which appear frequently in a corpus.\n\n# We use IDF to rescale the feature vectors\nidf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\nidfModel = idf.fit(featurizedData) # type is: pyspark.ml.feature.IDFModel\nrescaledData = idfModel.transform(featurizedData) # type: data frame\nprint type(rescaledData)\nfor features_label in rescaledData.select(\"features\", \"label\").take(3):\n    print(features_label)"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["####### VectorAssembler ########\n# VectorAssembler is a transformer that combines a given list of columns into a single vector column. It is useful for combining raw features and features generated by different feature transformers into a single feature vector, in order to train ML models like logistic regression and decision trees. \n\nfrom pyspark.mllib.linalg import Vectors\nfrom pyspark.ml.feature import VectorAssembler\n\ndataset = sqlContext.createDataFrame(\n    [(0, 18, 1.0, Vectors.dense([0.0, 10.0, 0.5]), 1.0)],\n    [\"id\", \"hour\", \"mobile\", \"userFeatures\", \"clicked\"])\n\nassembler = VectorAssembler(\n    inputCols=[\"hour\", \"mobile\", \"userFeatures\"],\n    outputCol=\"features\")\noutput = assembler.transform(dataset)\n#print type(output.select('features').collect())\noutput.show(truncate=False)"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":7}],"metadata":{"name":"Spark_ML","notebookId":3059045231783151},"nbformat":4,"nbformat_minor":0}
